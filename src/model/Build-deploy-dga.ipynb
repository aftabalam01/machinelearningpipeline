{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<h2>IMT 575: Sagemaker trial model</h2>\n",
    "<b><pre>\n",
    "    Authors: \n",
    "    Aftab Alam\n",
    "    </pre>\n",
    "</b> \n",
    "<p>Date/Time: <span id=\"datetime\"></span></p><script>var dt = new Date();\n",
    "document.getElementById(\"datetime\").innerHTML=dt.toLocaleString();</script> </p>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable flag to how all output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: tldextract in /opt/conda/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.22.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tldextract) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/conda/lib/python3.7/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "! conda install -y -c conda-forge ipywidgets\n",
    "! pip install tldextract\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri \n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-trial'\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 µs, sys: 1e+03 ns, total: 21 µs\n",
      "Wall time: 24.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split data and save in s3\n",
    "\n",
    "import io\n",
    "import boto3\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def appendXY(X,Y):\n",
    "    df = pd.DataFrame(X)\n",
    "    print(df.columns)\n",
    "    df.insert(loc=0,column='Y',value=Y)\n",
    "    return df\n",
    "   \n",
    "\n",
    "def data_split(features,output_label,train_file, validation_file, test_file,test_size=0.2, random_state=42):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split( features, output_label, test_size=test_size, random_state=random_state)\n",
    "    test_df = appendXY(X_test,y_test)\n",
    "    test_df.to_csv(test_file,header=False,index=False)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split( X_train, y_train, test_size=test_size, random_state=random_state)\n",
    "    train_df = appendXY(X_train,y_train)\n",
    "    train_df.to_csv(train_file,header=False,index=False)\n",
    "    \n",
    "    valid_df = appendXY(X_test,y_test)\n",
    "    valid_df.to_csv(validation_file,header=False,index=False)\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session(region_name=region).resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj=open(filename, 'rb')\n",
    "    key = prefix+'/'+channel\n",
    "    url = 's3://{}/{}/{}'.format(bucket, key, filename)\n",
    "    print('Writing to {}'.format(url))\n",
    "    write_to_s3(fobj, bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download file from S3\n",
    "FILE_DATA = 'domainsDataSet'\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, 'domainsDataSet.csv', FILE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1275047</th>\n",
       "      <td>8thdeadlysim.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275048</th>\n",
       "      <td>amiami.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275049</th>\n",
       "      <td>freedirectorywebsites.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275050</th>\n",
       "      <td>ghaninia.ir</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275051</th>\n",
       "      <td>gndoqarrd.dj</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            domain domain_type\n",
       "1275047           8thdeadlysim.com      benign\n",
       "1275048                 amiami.com      benign\n",
       "1275049  freedirectorywebsites.com      benign\n",
       "1275050                ghaninia.ir      benign\n",
       "1275051               gndoqarrd.dj         dga"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_domains = pd.read_csv(FILE_DATA)\n",
    "\n",
    "df_domains.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['domain', 'domain_type', 'domain_subdomain', 'Y'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_domains.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain         NaN\n",
      "domain_type    dga\n",
      "Name: 308868, dtype: object\n",
      "domain            NaN\n",
      "domain_type    benign\n",
      "Name: 940873, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import tldextract\n",
    "\n",
    "def extract_domain_subdomain(record):\n",
    "    domain = record.domain\n",
    "    ret=''\n",
    "    try:\n",
    "        ext = tldextract.extract(domain)\n",
    "        ret = ext.domain\n",
    "    except :\n",
    "        print(record)\n",
    "    return ret\n",
    "def get_y(row):\n",
    "    if row.domain_type.lower()=='dga':\n",
    "        return 1\n",
    "    elif row.domain_type.lower()=='benign':\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "    \n",
    "df_domains.loc[:,'domain_subdomain'] = df_domains.apply(lambda row : extract_domain_subdomain(row), axis=1 )\n",
    "## 1 for dga and 0 for benign\n",
    "df_domains.loc[:,'Y'] = df_domains.apply(lambda row : get_y(row), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(l, content, width):\n",
    "        l.extend([content] * (width - len(l)))\n",
    "        return l\n",
    "def features_extract(domain):\n",
    "    ch_list = []\n",
    "    for ch in domain :\n",
    "        ch_int = ord(ch)\n",
    "        ch_list = [*ch_list,ch_int]\n",
    "    # pad zeros up to 63 length\n",
    "    return pad(ch_list,0,63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tldextract example\n",
    "url = 'https://www.abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcde.abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.com'\n",
    "ext=tldextract.extract(url)\n",
    "len(features_extract(ext.domain))\n",
    "ext.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features X\n",
    "X = [features_extract(D) for D in df_domains.domain_subdomain]\n",
    "df = pd.DataFrame(X)\n",
    "df.insert(loc=0,column='Y',value=df_domains['Y'])\n",
    "df.to_csv('dataset.csv',header=False,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=63, step=1)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split df in training , validation and test and save as file\n",
    "FILE_TRAIN = 'domainsDataSet.train'\n",
    "FILE_VALIDATION = 'domainsDataSet.validation'\n",
    "FILE_TEST = 'domainsDataSet.test'\n",
    "\n",
    "data_split(features=X,output_label=df_domains['Y'],train_file=FILE_TRAIN, validation_file=FILE_VALIDATION, test_file=FILE_TEST,test_size=0.2, random_state=42)\n",
    "\n",
    "#upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, 'train', FILE_TRAIN)\n",
    "upload_to_s3(bucket, 'validation', FILE_VALIDATION)\n",
    "upload_to_s3(bucket, 'test', FILE_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-abalone-default'\n",
    "\n",
    "container = get_image_uri(region, 'xgboost','0.90-1')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=2,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        eval_metric='auc')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pickle-mixin\n",
    "!pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "path_key = f'{prefix}/output/sagemaker-xgboost-2020-05-07-12-31-15-641/output'\n",
    "# download the model artifact from AWS S3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, f'{path_key}/model.tar.gz', 'model.tar.gz')\n",
    "\n",
    "! ls -lrt\n",
    "\n",
    "\n",
    "#opens the downloaded model artifcat and loads it as 'model' variable\n",
    "tar = tarfile.open('model.tar.gz')\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "file = open('xgboost-model', 'rb')\n",
    "model = pkl.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
